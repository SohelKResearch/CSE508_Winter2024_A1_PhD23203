{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1** PreProcessing"
      ],
      "metadata": {
        "id": "5ILtyL0R76Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import os\n",
        "\n",
        "# Download necessary NLTK datasets\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_text(content):\n",
        "    # Lowercase the text\n",
        "    content = content.lower()\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(content)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Remove blank space tokens\n",
        "    tokens = [token for token in tokens if token.strip()]\n",
        "\n",
        "    # Rejoin tokens into a string\n",
        "    preprocessed_content = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_content\n",
        "\n",
        "# Assuming we have a list of file paths for the dataset\n",
        "dataset_directory = '/content/drive/MyDrive/text_files'  # Update this path\n",
        "file_paths = [os.path.join(dataset_directory, f) for f in os.listdir(dataset_directory) if f.endswith('.txt')][:5]  # Only take 5 sample files\n",
        "\n",
        "for file_path in file_paths:\n",
        "    # Read the content of each file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "    print(f\"Original content of {os.path.basename(file_path)}:\\n{content[:500]}\\n\")  # Print first 500 characters for brevity\n",
        "\n",
        "    # Preprocess the content\n",
        "    preprocessed_content = preprocess_text(content)\n",
        "\n",
        "    print(f\"Preprocessed content of {os.path.basename(file_path)}:\\n{preprocessed_content[:500]}\\n\")  # Print first 500 characters for brevity\n",
        "\n",
        "    # Save the preprocessed content back to a new file\n",
        "    preprocessed_file_path = os.path.join(dataset_directory, 'preprocessed_' + os.path.basename(file_path))\n",
        "    with open(preprocessed_file_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(preprocessed_content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oU0pHVpR7uxb",
        "outputId": "1a94608a-64f8-44c7-932d-9dc011feaceb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original content of file511.txt:\n",
            "I loved using this while producing videos with my Canon EOS 6D, but it didn't last long.  When I inserted the battery into the receiver, one of the battery contacts broke off.  Here are some pictures of the receiver mounted on my EOS 6D and a picture showing the broken contact wire.  The battery holder has a bad design because the contacts are weak wire loops and the battery fits very snug and puts a lot of pressure on these weak contacts.  Samson should update the battery holder design.  I stil\n",
            "\n",
            "Preprocessed content of file511.txt:\n",
            "loved using producing videos canon eos 6d n't last long inserted battery receiver one battery contacts broke pictures receiver mounted eos 6d picture showing broken contact wire battery holder bad design contacts weak wire loops battery fits snug puts lot pressure weak contacts samson update battery holder design still recommend wonderful wireless system something aware\n",
            "\n",
            "Original content of file209.txt:\n",
            "Love it!!! Would buy again!\n",
            "\n",
            "Preprocessed content of file209.txt:\n",
            "love would buy\n",
            "\n",
            "Original content of file657.txt:\n",
            "This is for my son in his rehearsal room and they are perfect! Great quality, arrived within a few days and the price you cant beat!\n",
            "\n",
            "Preprocessed content of file657.txt:\n",
            "son rehearsal room perfect great quality arrived within days price cant beat\n",
            "\n",
            "Original content of file172.txt:\n",
            "These stands are excellent. They are well built and easy to assemble. they come with an Allen wrench and that is the only tool needed. The stand has a nice design when it comes to the base. There are 3 soft rubber pads that come on the stand. They also give you 3 pointed metal spikes that you can put on, which is perfect if placing on a rug. The spikes are adjustable so you can get set for the rug height and get the stand perfectly level. I put a pair of JBL monitor speakers (LSR305) on this sta\n",
            "\n",
            "Preprocessed content of file172.txt:\n",
            "stands excellent well built easy assemble come allen wrench tool needed stand nice design comes base 3 soft rubber pads come stand also give 3 pointed metal spikes put perfect placing rug spikes adjustable get set rug height get stand perfectly level put pair jbl monitor speakers lsr305 stand yamaha piano dgx-650b see attached pictures stands look like made go piano quality metal good despite reviews pins allow lock stand certain height n't worry lowering comes wire guides run cables tight stand\n",
            "\n",
            "Original content of file366.txt:\n",
            "Overall I'm happy with the stand.  I admit I was scratching my head a bit when I was putting it together.  There's more pieces to this stand than others I've had.  Adjusting it is a little bit of a pain because there are 4 different areas that you can loosen and change around.  If you travel and gig a lot, you may want to go with the higher model, the adjustments look a bit easier.  However once all setup, it's sturdy and well built.\n",
            "\n",
            "FYI the stand shown currently is not the same design as the o\n",
            "\n",
            "Preprocessed content of file366.txt:\n",
            "overall 'm happy stand admit scratching head bit putting together 's pieces stand others 've adjusting little bit pain 4 different areas loosen change around travel gig lot may want go higher model adjustments look bit easier however setup 's sturdy well built fyi stand shown currently design one received uploaded picture one received order page separate arm piece inserts main stand\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q2** Inverted_Index"
      ],
      "metadata": {
        "id": "IpXVtMp48Fii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import pickle\n",
        "from tqdm.notebook import tqdm\n",
        "import string\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "from functools import reduce\n",
        "import copy\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "class TextIndexer:\n",
        "    def __init__(self):\n",
        "        self.index = {}\n",
        "\n",
        "    def removeSpecialChars(self, text):\n",
        "        return ''.join(c for c in text if c.isalnum() and not c.isdigit() and c not in string.punctuation)\n",
        "\n",
        "    def processText(self, text):\n",
        "        filteredWords = set(stopwords.words('english'))\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [i for i in tokens if i not in filteredWords]\n",
        "        tokens = [self.removeSpecialChars(x) for x in tokens]\n",
        "        return tokens\n",
        "\n",
        "    def addToIndex(self, content, contentID):\n",
        "        words = self.processText(content)\n",
        "        for position, word in enumerate(words):\n",
        "            if word in self.index:\n",
        "                self.index[word][0] += 1\n",
        "                if contentID in self.index[word][1]:\n",
        "                    self.index[word][1][contentID].append(position)\n",
        "                else:\n",
        "                    self.index[word][1][contentID] = [position]\n",
        "            else:\n",
        "                self.index[word] = [1, {contentID: [position]}]\n",
        "\n",
        "    def storeIndex(self, filename='index_output.pickle'):\n",
        "        with open(filename, 'wb') as file:\n",
        "            pickle.dump(self.index, file)\n",
        "\n",
        "class SearchQuery:\n",
        "    def __init__(self, index_file='index_output.pickle', mapping_file='file_mapping.pickle'):\n",
        "        with open(index_file, 'rb') as file:\n",
        "            self.index = pickle.load(file)\n",
        "        self.index = defaultdict(lambda: [], self.index)\n",
        "        with open(mapping_file, 'rb') as file:\n",
        "            self.mapping = pickle.load(file)\n",
        "\n",
        "    def removeSpecialChars(self, text):\n",
        "        return ''.join(c for c in text if c.isalnum() and not c.isdigit() and c not in string.punctuation)\n",
        "\n",
        "    def processText(self, text):\n",
        "        filteredWords = set(stopwords.words('english'))\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [i for i in tokens if i not in filteredWords]\n",
        "        tokens = [self.removeSpecialChars(x) for x in tokens]\n",
        "        tokens = [x for x in tokens if len(x) > 1]\n",
        "        return tokens\n",
        "\n",
        "    def listIntersection(self, lists):\n",
        "        if not lists:\n",
        "            return []\n",
        "        lists.sort(key=len)\n",
        "        return list(reduce(lambda x, y: set(x) & set(y), lists))\n",
        "\n",
        "    def getTermPostings(self, terms):\n",
        "        return [[ [docID, self.index[term][1][docID]] for docID in self.index[term][1]] for term in terms]\n",
        "\n",
        "    def extractDocIDs(self, postings):\n",
        "        return [[item[0] for item in term] for term in postings]\n",
        "\n",
        "    def query(self, queryText):\n",
        "        terms = self.processText(queryText)\n",
        "        if not self.index.keys():\n",
        "            return []\n",
        "        for term in terms:\n",
        "            if term not in self.index:\n",
        "                return []\n",
        "        if len(terms) == 1:\n",
        "            docIDs = list(self.index[terms[0]][1].keys())\n",
        "        else:\n",
        "            postings = self.getTermPostings(terms)\n",
        "            docs = self.extractDocIDs(postings)\n",
        "            docs = self.listIntersection(docs)\n",
        "            for i in range(len(postings)):\n",
        "                postings[i] = [x for x in postings[i] if x[0] in docs]\n",
        "            postings = copy.deepcopy(postings)\n",
        "            for i in range(len(postings)):\n",
        "                for j in range(len(postings[i])):\n",
        "                    postings[i][j][1] = [pos - i for pos in postings[i][j][1]]\n",
        "            docIDs = []\n",
        "            for i in range(len(postings[0])):\n",
        "                commonPos = self.listIntersection([x[i][1] for x in postings])\n",
        "                if commonPos:\n",
        "                    docIDs.append(postings[0][i][0])\n",
        "        docIDs = list(map(int, docIDs))\n",
        "        return docIDs\n",
        "\n",
        "    def displayFiles(self, docIDs):\n",
        "        files = sorted([(docID, self.mapping[docID]) for docID in docIDs])\n",
        "        return files\n",
        "\n",
        "def main():\n",
        "    indexer = TextIndexer()\n",
        "    fileList = []\n",
        "    dataset_directory = '/content/drive/MyDrive/text_files'\n",
        "    fileList = [os.path.join(dataset_directory, f) for f in os.listdir(dataset_directory) if f.endswith('.txt')]\n",
        "\n",
        "    mapping = {}\n",
        "\n",
        "    for i, path in enumerate(fileList):\n",
        "        try:\n",
        "            content = open(path, encoding=\"utf8\").read().replace('\\n', ' ')\n",
        "        except Exception:\n",
        "            content = open(path, encoding=\"unicode_escape\").read().replace('\\n', ' ')\n",
        "        indexer.addToIndex(content, i)\n",
        "        mapping[i] = path\n",
        "\n",
        "    indexer.storeIndex()\n",
        "    with open('file_mapping.pickle', 'wb') as file:\n",
        "        pickle.dump(mapping, file)\n",
        "\n",
        "    queryEngine = SearchQuery()\n",
        "\n",
        "    num_queries = int(input(\"Enter number of queries: \"))\n",
        "    for i in range(num_queries):\n",
        "        queryText = input(f\"Enter query {i+1}: \")\n",
        "        results = queryEngine.query(queryText)\n",
        "        files = queryEngine.displayFiles(results)\n",
        "        print(f\"Number of documents retrieved for query {i+1} using positional index: {len(results)}\")\n",
        "        if files:\n",
        "            print(f\"Names of documents retrieved for query {i+1} using positional index: {', '.join([f[1] for f in files])}\")\n",
        "        else:\n",
        "            print(\"No documents retrieved\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDgH1IOfzuYp",
        "outputId": "85a7f675-af31-4777-f581-b436e693f3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of queries: 2\n",
            "Enter query 1: Car bag in a canister\n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "No documents retrieved\n",
            "Enter query 2:  Coffee brewing techniques in cookbook\n",
            "Number of documents retrieved for query 2 using positional index: 0\n",
            "No documents retrieved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Q3** Positional_Index"
      ],
      "metadata": {
        "id": "lBl0dIqk8VID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from functools import reduce\n",
        "import copy\n",
        "\n",
        "# Ensure NLTK resources have been downloaded\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "\n",
        "class IndexCreator:\n",
        "    def __init__(self):\n",
        "        self.indices = {}\n",
        "\n",
        "    def cleanText(self, content):\n",
        "        return ''.join(c for c in content if c.isalnum() and not c.isdigit() and c not in string.punctuation)\n",
        "\n",
        "    def processContent(self, content):\n",
        "        ignoreWords = set(stopwords.words('english'))\n",
        "        content = content.lower()\n",
        "        tokens = word_tokenize(content)\n",
        "        tokens = [i for i in tokens if i not in ignoreWords]\n",
        "        tokens = [self.cleanText(x) for x in tokens]\n",
        "        return tokens\n",
        "\n",
        "    def buildIndex(self, text, docID):\n",
        "        words = self.processContent(text)\n",
        "        for pos, term in enumerate(words):\n",
        "            if term in self.indices:\n",
        "                self.indices[term][0] += 1\n",
        "                if docID in self.indices[term][1]:\n",
        "                    self.indices[term][1][docID].append(pos)\n",
        "                else:\n",
        "                    self.indices[term][1][docID] = [pos]\n",
        "            else:\n",
        "                self.indices[term] = [1, {docID: [pos]}]\n",
        "\n",
        "    def saveIndices(self, filename='indices.pkl'):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self.indices, f)\n",
        "\n",
        "class QueryProcessor:\n",
        "    def __init__(self, indexFile='indices.pkl', docMappingFile='docMapping.pkl'):\n",
        "        with open(indexFile, 'rb') as f:\n",
        "            self.indices = pickle.load(f)\n",
        "        with open(docMappingFile, 'rb') as f:\n",
        "            self.docMapping = pickle.load(f)\n",
        "\n",
        "    def search(self, queryTerms, logicOps):\n",
        "        docSets = []\n",
        "        for term in queryTerms:\n",
        "            if term in self.indices:\n",
        "                docIDs = set(self.indices[term][1].keys())  # Get document IDs containing the term\n",
        "                docSets.append(docIDs)\n",
        "            else:\n",
        "                return []  # If any term is not found, return an empty list\n",
        "\n",
        "        # Find the intersection of all document ID sets to get IDs containing all terms\n",
        "        if docSets:\n",
        "            resultIDs = set.intersection(*docSets)\n",
        "            return list(resultIDs)\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "    def displayResults(self, docIDs):\n",
        "        if docIDs is None:  # Handle the case where docIDs is None\n",
        "            return []\n",
        "        docNames = [self.docMapping[str(id)] for id in docIDs]  # Ensure IDs are converted to strings if necessary\n",
        "        return docNames\n",
        "\n",
        "\n",
        "def main():\n",
        "    dataset_directory = '/content/drive/MyDrive/text_files'  # Adjust this path to your dataset directory\n",
        "    fileList = [os.path.join(dataset_directory, f) for f in os.listdir(dataset_directory) if f.endswith('.txt')]\n",
        "    docMapping = {i: name for i, name in enumerate(fileList)}\n",
        "\n",
        "    ic = IndexCreator()\n",
        "    for docID, filePath in enumerate(fileList):\n",
        "        with open(filePath, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        ic.buildIndex(content, docID)\n",
        "    ic.saveIndices()\n",
        "\n",
        "    # Save the document mapping\n",
        "    with open('docMapping.pkl', 'wb') as f:\n",
        "        pickle.dump(docMapping, f)\n",
        "\n",
        "    qp = QueryProcessor()\n",
        "\n",
        "    numQueries = int(input(\"Enter the number of queries: \"))\n",
        "    for i in range(numQueries):\n",
        "        query = input(f\"Enter query {i+1}: \")\n",
        "        logic = input(\"Enter logic operators: \")\n",
        "        queryTerms = query.split(', ')\n",
        "        logicOps = logic.split(', ')\n",
        "        docIDs = qp.search(queryTerms, logicOps) # You need to implement the search logic based on queryTerms and logicOps\n",
        "        print(f\"Query {i+1}: {' '.join(queryTerms)}\")\n",
        "        # Assume docIDs is defined after implementing search logic\n",
        "        docNames = qp.displayResults(docIDs)\n",
        "        print(f\"Names of the documents retrieved for query {i+1}: {', '.join(docNames)}\")\n",
        "        # Placeholder for search logic demonstration\n",
        "        print(\"Search logic to be implemented\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "qPIJzE391f8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f45be49-6ec8-40ea-f806-f646f2b0dc7e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter query 1: Car bag in a canister\n",
            "Enter logic operators: OR, AND NOT\n",
            "Query 1: Car bag in a canister\n",
            "Names of the documents retrieved for query 1: \n",
            "Search logic to be implemented\n",
            "Enter query 2:  Coffee brewing techniques in cookbook\n",
            "Enter logic operators: AND, OR NOT,OR\n",
            "Query 2:  Coffee brewing techniques in cookbook\n",
            "Names of the documents retrieved for query 2: \n",
            "Search logic to be implemented\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "497vUzeP3ubb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}